@article{candadai2020sources,
  title={Sources of predictive information in dynamical neural networks},
  author={Candadai, Madhavun and Izquierdo, Eduardo J},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={16901},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{guttulsrud2023solving,
  title={Solving the Lunar Lander Problem with Multiple Uncertainties using a Deep Q-Learning based Short-Term Memory Agent},
  author={Guttulsrud, H{\aa}kon and Sandnes, Mathias and Shrestha, Raju},
  booktitle={Proceedings of the 2023 12th International Conference on Computing and Pattern Recognition},
  pages={27--33},
  year={2023}
}

@article{keneshloo2019deep,
  title={Deep reinforcement learning for sequence-to-sequence models},
  author={Keneshloo, Yaser and Shi, Tian and Ramakrishnan, Naren and Reddy, Chandan K},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={7},
  pages={2469--2489},
  year={2019},
  publisher={IEEE}
}

@article{parsons2015virtual,
  title={Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences},
  author={Parsons, Thomas D},
  journal={Frontiers in human neuroscience},
  volume={9},
  pages={660},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{zhang2020cddpg,
  title={CDDPG: A deep-reinforcement-learning-based approach for electric vehicle charging control},
  author={Zhang, Feiye and Yang, Qingyu and An, Dou},
  journal={IEEE Internet of Things Journal},
  volume={8},
  number={5},
  pages={3075--3087},
  year={2020},
  publisher={IEEE}
}

@inproceedings{eberhard2022pink,
  title={Pink noise is all you need: Colored noise exploration in deep reinforcement learning},
  author={Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{Gjersvik2019LandingMoon,
  author    = {Adam Gjersvik},
  title     = {Landing on the Moon with Deep Deterministic Policy Gradients},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},  
  year      = {2019},
  volume    = {31},  
  pages     = {2469--2489},  
  abstract  = {Ask Neil Armstrong, landing on the Moon is no easy feat - which is why ideally we would like to have computers do it for us. This work explores an extremely simplified lunar environment and attempts to train a computerized agent to perform a successful moon landing using a Deep Deterministic Policy Gradient (DDPG) algorithm.},
  keywords  = {Deep Deterministic Policy Gradient, Q-Learning, Decision Making Under Uncertainty}
}

@misc{openaiTwinDelayed,
	author = {},
	title = {{T}win {D}elayed {D}{D}{P}{G} \&#x2014; {S}pinning {U}p documentation --- spinningup.openai.com},
	howpublished = {\url{https://spinningup.openai.com/en/latest/algorithms/td3.html}},
	year = {},
	note = {[Accessed 14-05-2024]},
}

@inproceedings{di2022analysis,
  title={Analysis of stochastic processes through replay buffers},
  author={Di-Castro, Shirli and Mannor, Shie and Di Castro, Dotan},
  booktitle={International Conference on Machine Learning},
  pages={5039--5060},
  year={2022},
  organization={PMLR}
}

@inproceedings{guttulsrud2023solving,
  title={Solving the Lunar Lander Problem with Multiple Uncertainties using a Deep Q-Learning based Short-Term Memory Agent},
  author={Guttulsrud, H{\aa}kon and Sandnes, Mathias and Shrestha, Raju},
  booktitle={Proceedings of the 2023 12th International Conference on Computing and Pattern Recognition},
  pages={27--33},
  year={2023}
}

@misc{arxivContinuousControl,
	author = {},
	title = {{C}ontinuous control with deep reinforcement learning --- arxiv.org},
	year = {},
	note = {[Accessed 14-05-2024]},
}

@article{ganesh2022review,
  title={A review of reinforcement learning based energy management systems for electrified powertrains: Progress, challenge, and potential solution},
  author={Ganesh, Akhil Hannegudda and Xu, Bin},
  journal={Renewable and Sustainable Energy Reviews},
  volume={154},
  pages={111833},
  year={2022},
  publisher={Elsevier}
}

@book{palanisamy2018hands,
  title={Hands-On Intelligent Agents with OpenAI Gym: Your guide to developing AI agents using deep reinforcement learning},
  author={Palanisamy, Praveen},
  year={2018},
  publisher={Packt Publishing Ltd}
}

@article{han2019energy,
  title={Energy management based on reinforcement learning with double deep Q-learning for a hybrid electric tracked vehicle},
  author={Han, Xuefeng and He, Hongwen and Wu, Jingda and Peng, Jiankun and Li, Yuecheng},
  journal={Applied Energy},
  volume={254},
  pages={113708},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{tang2020discretizing,
  title={Discretizing continuous action space for on-policy optimization},
  author={Tang, Yunhao and Agrawal, Shipra},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={34},
  number={04},
  pages={5981--5988},
  year={2020}
}

@inproceedings{elkins2020adaptive,
  title={Adaptive continuous control of spacecraft attitude using deep reinforcement learning},
  author={Elkins, Jacob and Sood, Rohan and Rumpf, Clemens},
  booktitle={2020 AAS/AIAA Astrodynamics Specialist Conference},
  pages={420--475},
  year={2020},
  organization={AIAA Reston, VA}
}

@article{dulac2021challenges,
  title={Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
  author={Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  journal={Machine Learning},
  volume={110},
  number={9},
  pages={2419--2468},
  year={2021},
  publisher={Springer}
}

@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@article{hazrathosseini2024transition,
  title={Transition to intelligent fleet management systems in open pit mines: A critical review on application of reinforcement-learning-based systems},
  author={Hazrathosseini, Arman and Moradi Afrapoli, Ali},
  journal={Mining Technology},
  volume={133},
  number={1},
  pages={50--73},
  year={2024},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{goudarzi2021distributed,
  title={A distributed deep reinforcement learning technique for application placement in edge and fog computing environments},
  author={Goudarzi, Mohammad and Palaniswami, Marimuthu and Buyya, Rajkumar},
  journal={IEEE Transactions on Mobile Computing},
  volume={22},
  number={5},
  pages={2491--2505},
  year={2021},
  publisher={IEEE}
}

@article{hossny2021refined,
  title={Refined continuous control of ddpg actors via parametrised activation},
  author={Hossny, Mohammed and Iskander, Julie and Attia, Mohamed and Saleh, Khaled and Abobakr, Ahmed},
  journal={AI},
  volume={2},
  number={4},
  pages={464--476},
  year={2021},
  publisher={MDPI}
}

@inproceedings{liu2018effects,
  title={The effects of memory replay in reinforcement learning},
  author={Liu, Ruishan and Zou, James},
  booktitle={2018 56th annual allerton conference on communication, control, and computing (Allerton)},
  pages={478--485},
  year={2018},
  organization={IEEE}
}

@article{zhong2019deep,
  title={A deep actor-critic reinforcement learning framework for dynamic multichannel access},
  author={Zhong, Chen and Lu, Ziyang and Gursoy, M Cenk and Velipasalar, Senem},
  journal={IEEE Transactions on Cognitive Communications and Networking},
  volume={5},
  number={4},
  pages={1125--1139},
  year={2019},
  publisher={IEEE}
}



@article{sung2017learning,
  title={Learning to learn: Meta-critic networks for sample efficient learning},
  author={Sung, Flood and Zhang, Li and Xiang, Tao and Hospedales, Timothy and Yang, Yongxin},
  journal={arXiv preprint arXiv:1706.09529},
  year={2017}
}
